{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"../../datasets/classification/train.csv\")\n",
    "test = pd.read_csv(\"../../datasets/classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = []\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[train[col].isna()].shape[0] > 0:\n",
    "        na_cols.append(col)\n",
    "\n",
    "train = train.dropna(axis=0, subset=na_cols)\n",
    "\n",
    "X_train = train.drop(\"satisfaction\", axis = 1)\n",
    "y_train = train[\"satisfaction\"]\n",
    "\n",
    "na_cols = []\n",
    "\n",
    "for col in test.columns:\n",
    "    if test[test[col].isna()].shape[0] > 0:\n",
    "        na_cols.append(col)\n",
    "\n",
    "test = test.dropna(axis=0, subset=na_cols)\n",
    "\n",
    "X_test = test.drop(\"satisfaction\", axis = 1)\n",
    "y_test = test[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"Gender\"] = X_train[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n",
    "X_train[\"Customer Type\"] = X_train[\"Customer Type\"].map({\"Loyal Customer\": 0, \"disloyal Customer\": 1})\n",
    "X_train[\"Type of Travel\"] = X_train[\"Type of Travel\"].map({\"Personal Travel\": 0, \"Business travel\": 1})\n",
    "X_train[\"Class\"] = X_train[\"Class\"].map({\"Eco Plus\": 0, \"Business\": 1, \"Eco\": 2})\n",
    "\n",
    "X_test[\"Gender\"] = X_test[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n",
    "X_test[\"Customer Type\"] = X_test[\"Customer Type\"].map({\"Loyal Customer\": 0, \"disloyal Customer\": 1})\n",
    "X_test[\"Type of Travel\"] = X_test[\"Type of Travel\"].map({\"Personal Travel\": 0, \"Business travel\": 1})\n",
    "X_test[\"Class\"] = X_test[\"Class\"].map({\"Eco Plus\": 0, \"Business\": 1, \"Eco\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lbls = [\n",
    "    'dt', \n",
    "    'nb', \n",
    "    'lp', \n",
    "    'svc', \n",
    "    'knn'\n",
    "]\n",
    "\n",
    "# Set the parameters to be explored by the grid for each classifier\n",
    "tuned_param_dt = [{'max_depth': list(range(1,20))}]\n",
    "tuned_param_nb = [{'var_smoothing': [1e-5]}]#[10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [\n",
    "    # {'kernel': ['rbf'], \n",
    "    #                 'gamma': [1e-3, 1e-4],\n",
    "    #                 'C': [1, 10, 100, 1000],\n",
    "    #                 },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [100]#[1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [5]}]#, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "# set the models to be fitted specifying name, estimator and parameter structure\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "# scores to be explored\n",
    "scores = [\n",
    "    \"precision\", \n",
    "    \"recall\",\n",
    "    \"f1\",\n",
    "    \"accuracy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Decision Tree       \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'max_depth': 14}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.787 (+/-0.003) for {'max_depth': 1}\n",
      "0.857 (+/-0.006) for {'max_depth': 2}\n",
      "0.882 (+/-0.003) for {'max_depth': 3}\n",
      "0.899 (+/-0.004) for {'max_depth': 4}\n",
      "0.906 (+/-0.002) for {'max_depth': 5}\n",
      "0.910 (+/-0.005) for {'max_depth': 6}\n",
      "0.928 (+/-0.004) for {'max_depth': 7}\n",
      "0.938 (+/-0.002) for {'max_depth': 8}\n",
      "0.938 (+/-0.010) for {'max_depth': 9}\n",
      "0.941 (+/-0.012) for {'max_depth': 10}\n",
      "0.944 (+/-0.015) for {'max_depth': 11}\n",
      "0.945 (+/-0.021) for {'max_depth': 12}\n",
      "0.945 (+/-0.026) for {'max_depth': 13}\n",
      "0.946 (+/-0.021) for {'max_depth': 14}\n",
      "0.945 (+/-0.024) for {'max_depth': 15}\n",
      "0.942 (+/-0.028) for {'max_depth': 16}\n",
      "0.942 (+/-0.026) for {'max_depth': 17}\n",
      "0.937 (+/-0.040) for {'max_depth': 18}\n",
      "0.938 (+/-0.034) for {'max_depth': 19}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "neutral or dissatisfied       0.95      0.96      0.96     14528\n",
      "              satisfied       0.95      0.94      0.95     11365\n",
      "\n",
      "               accuracy                           0.95     25893\n",
      "              macro avg       0.95      0.95      0.95     25893\n",
      "           weighted avg       0.95      0.95      0.95     25893\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Gaussian Naive Bayes\n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'var_smoothing': 1e-05}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.666 (+/-0.006) for {'var_smoothing': 1e-05}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "neutral or dissatisfied       0.64      0.87      0.74     14528\n",
      "              satisfied       0.69      0.38      0.49     11365\n",
      "\n",
      "               accuracy                           0.65     25893\n",
      "              macro avg       0.67      0.62      0.61     25893\n",
      "           weighted avg       0.66      0.65      0.63     25893\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Linear Perceptron   \n",
      "Best parameters set found on train set:\n",
      "\n",
      "{'early_stopping': True}\n",
      "\n",
      "Grid scores on train set:\n",
      "\n",
      "0.437 (+/-0.363) for {'early_stopping': True}\n",
      "\n",
      "Detailed classification report for the best parameter set:\n",
      "\n",
      "The model is trained on the full train set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "neutral or dissatisfied       0.66      0.45      0.53     14528\n",
      "              satisfied       0.50      0.71      0.59     11365\n",
      "\n",
      "               accuracy                           0.56     25893\n",
      "              macro avg       0.58      0.58      0.56     25893\n",
      "           weighted avg       0.59      0.56      0.56     25893\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "Trying model Support Vector      \n"
     ]
    }
   ],
   "source": [
    "results_short = {}\n",
    "\n",
    "for score in scores:\n",
    "    print('='*40)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls:\n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                           scoring=('%s_macro' % score) if score != \"accuracy\" else score, \n",
    "#                            iid = False, \n",
    "                           return_train_score = False,\n",
    "                           n_jobs = 2, # this allows using multi-cores\n",
    "                           )\n",
    "        clf.fit(X_train, y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:5.2f}%\".format(models[m]['name'], results_short[m]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
